{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "This tutorial illustrates how to fine-tune the `GR00T-N1` pretrained checkpoint on a post-training \n",
    "dataset using the same embodiment. This showcases the benefit of post-training, transforming a generalist model into a specialist and demonstrating a performance gain.\n",
    "\n",
    "For this tutorial, we will use the demo dataset `robot_sim.PickNPlace` from the [demo_data](./demo_data) folder. \n",
    "\n",
    "We will first load the pre-trained model and evaluate it on the dataset. Then we will finetune the model on the dataset and evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.eval import calc_mse_for_single_trajectory\n",
    "import warnings\n",
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "from gr00t.model.policy import Gr00tPolicy\n",
    "from gr00t.data.schema import EmbodimentTag\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_PATH = \"nvidia/GR00T-N1-2B\"\n",
    "EMBODIMENT_TAG = EmbodimentTag.GR1\n",
    "DATASET_PATH = \"../demo_data/robot_sim.PickNPlace\"\n",
    "\n",
    "\n",
    "data_config = DATA_CONFIG_MAP[\"gr1_arms_only\"]\n",
    "modality_config = data_config.modality_config()\n",
    "modality_transform = data_config.transform()\n",
    "\n",
    "\n",
    "pre_trained_policy = Gr00tPolicy(\n",
    "    model_path=PRE_TRAINED_MODEL_PATH,\n",
    "    embodiment_tag=EMBODIMENT_TAG,\n",
    "    modality_config=modality_config,\n",
    "    modality_transform=modality_transform,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    modality_configs=modality_config,\n",
    "    video_backend=\"decord\",\n",
    "    video_backend_kwargs=None,\n",
    "    transforms=None,  # We'll handle transforms separately through the policy\n",
    "    embodiment_tag=EMBODIMENT_TAG,\n",
    ")\n",
    "\n",
    "\n",
    "mse = calc_mse_for_single_trajectory(\n",
    "    pre_trained_policy,\n",
    "    dataset,\n",
    "    traj_id=0,\n",
    "    modality_keys=[\"right_arm\", \"right_hand\"],   # we will only evaluate the right arm and right hand\n",
    "    steps=150,\n",
    "    action_horizon=16,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(\"MSE loss for trajectory 0:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we can see the predicted actions and the ground truth actions. The predicted actions are not perfect but they are close to the ground truth actions. That's show that the pretrained checkpoint is working well.\n",
    "\n",
    "Now let's sample 10 random trajectories and calcuate the mean MSE to get a sense of more verbose results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trajectories = len(dataset.trajectory_lengths)\n",
    "\n",
    "print(\"Total trajectories:\", total_trajectories)\n",
    "\n",
    "sampled_trajectories = np.random.choice(total_trajectories, 10)\n",
    "print(\"Sampled trajectories:\", sampled_trajectories)\n",
    "\n",
    "all_mses = []\n",
    "\n",
    "for traj_id in sampled_trajectories:\n",
    "    mse = calc_mse_for_single_trajectory(\n",
    "        pre_trained_policy,\n",
    "        dataset,\n",
    "        traj_id=traj_id,\n",
    "        modality_keys=[\"right_arm\", \"right_hand\"],   # we will only evaluate the right arm and right hand\n",
    "        steps=150,\n",
    "        action_horizon=16,\n",
    "        plot=False\n",
    "    )\n",
    "    print(f\"Trajectory {traj_id} MSE: {mse:.4f}\")\n",
    "    \n",
    "    all_mses.append(mse)\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"Mean MSE:\", np.mean(all_mses))\n",
    "print(\"Std MSE:\", np.std(all_mses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the Model\n",
    "\n",
    "Now we will finetune the model on the dataset. Without going into the details of the finetuning process, we will use the `gr00t_finetune.py` script to finetune the model. You can run the following command to finetune the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python scripts/gr00t_finetune.py --dataset-path ./demo_data/robot_sim.PickNPlace --num-gpus 1 --max-steps 500 --output-dir /tmp/gr00t-1/finetuned-model --data-config gr1_arms_only\n",
    "```\n",
    "\n",
    "_To get a full list of the available arguments, you can run `python scripts/gr00t_finetune.py --help`._\n",
    "\n",
    "The script will save the finetuned model in the `/tmp/gr00t-1/finetuned-model` directory. We will load the finetuned model with `500` checkpoint steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Fine-tuned Model\n",
    "\n",
    "Now we can evaluate the fine-tuned model by running the policy on the dataset and see how well it performs. We will use a utility function to evaluate the policy on the dataset. This is similar to the previous tutorial in [1_pretrained_model.ipynb](1_pretrained_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.eval import calc_mse_for_single_trajectory\n",
    "import warnings\n",
    "\n",
    "finetuned_model_path = \"/tmp/gr00t-1/finetuned-model/checkpoint-500\"\n",
    "finetuned_policy = Gr00tPolicy(\n",
    "    model_path=finetuned_model_path,\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    modality_config=modality_config,\n",
    "    modality_transform=modality_transform,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "mse = calc_mse_for_single_trajectory(\n",
    "    finetuned_policy,\n",
    "    dataset,\n",
    "    traj_id=0,\n",
    "    modality_keys=[\"right_arm\", \"right_hand\"],   # we will only evaluate the right arm and right hand\n",
    "    steps=150,\n",
    "    action_horizon=16,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(\"MSE loss for trajectory 0:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaay! We have finetuned the model and evaluated it on the dataset. We can see that the model has learned the task and is able to perform the task better than the pre-trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groot-release",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
