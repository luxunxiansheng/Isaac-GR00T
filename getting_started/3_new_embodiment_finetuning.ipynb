{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Embodiment Finetuning Tutorial\n",
    "\n",
    "This notebook is a tutorial on how to finetune GR00T-N1 pretrained model on a new dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lerobot SO100 Finetuning Tutorial\n",
    "\n",
    "GR00T-N1 is accessible to everyone with various robot form-factors. Based on Huggingface's low-cost [So100 Lerobot arm](https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md), users can finetune GR00T-N1 on their own robot via a `new_embodiment` tag.\n",
    "\n",
    "\n",
    "![so100_eval_demo.gif](../media/so100_eval_demo.gif)\n",
    "\n",
    "## Step 1: Dataset\n",
    "\n",
    "Users can use any lerobot dataset for finetuning. In this tutorial, we will first use a sample dataset: [so100_strawberry_grape](https://huggingface.co/spaces/lerobot/visualize_dataset?dataset=youliangtan%2Fso100_strawberry_grape&episode=0)\n",
    "\n",
    "Note that this embodiment was not used in our pretraining dataset mixture.\n",
    "\n",
    "\n",
    "### First, download the dataset\n",
    "\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset youliangtan/so100_strawberry_grape --local-dir ./demo_data/so100_strawberry_grape\n",
    "```\n",
    "\n",
    "### Second, copy over the modality file\n",
    "\n",
    "The `modality.json` file provides additional information about the state and action modalities to make it \"GR00T-compatible\". Copy over the `examples/so100__modality.json` to the dataset `<DATASET_PATH>/meta/modality.json`.\n",
    "\n",
    "```bash\n",
    "cp examples/so100__modality.json ./demo_data/so100_strawberry_grape/meta/modality.json\n",
    "```\n",
    "\n",
    "Then we can load the dataset using the `LeRobotSingleDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.misc import any_describe\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "\n",
    "dataset_path = \"/workspaces/Isaac-GR00T/demo_data/so100_strawberry_grape\"   # change this to your dataset path\n",
    "\n",
    "data_config = DATA_CONFIG_MAP[\"so100\"]\n",
    "\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=data_config.modality_config(),\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n",
    "\n",
    "resp = dataset[7]\n",
    "any_describe(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "# show img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        resp = dataset[i]\n",
    "        img = resp[\"video.webcam\"][0]\n",
    "        images_list.append(img)\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images_list[i])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i}\")\n",
    "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Finetuning\n",
    "\n",
    "Finetuning can be done by using our finetuning `scripts/gr00t_finetune.py`.\n",
    "\n",
    "\n",
    "```bash\n",
    "python scripts/gr00t_finetune.py \\\n",
    "   --dataset-path /datasets/so100_strawberry_grape/ \\\n",
    "   --num-gpus 1 \\\n",
    "   --output-dir ~/so100-checkpoints  \\\n",
    "   --max-steps 2000 \\\n",
    "   --data-config so100 \\\n",
    "   --video-backend torchvision_av\n",
    "```\n",
    "\n",
    "## Step 3: Open-loop evaluation\n",
    "\n",
    "Once the training is done, you can run the following command to visualize the finetuned policy. \n",
    "\n",
    "```bash\n",
    "python scripts/eval_policy.py --plot \\\n",
    "   --embodiment_tag new_embodiment \\\n",
    "   --model_path <YOUR_CHECKPOINT_PATH> \\\n",
    "   --data_config so100 \\\n",
    "  --dataset_path /datasets/so100_strawberry_grape/ \\\n",
    "   --video_backend torchvision_av \\\n",
    "   --modality_keys single_arm gripper\n",
    "```\n",
    "\n",
    "This is a plot after training the policy for 7k steps.\n",
    "\n",
    "![so100-7k-steps.png](../media/so100-7k-steps.png)\n",
    "\n",
    "\n",
    "After training for more steps the plot will look significantly better.\n",
    "\n",
    "\n",
    "TADA! You have successfully finetuned GR00T-N1 on a new embodiment.\n",
    "\n",
    "## Deployment\n",
    "\n",
    "For more details about deployment, please refer to the notebook: `5_policy_deployment.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. G1 Block Stacking Dataset Finetuning Tutorial\n",
    "\n",
    "This provides a step-by-step guide on how to finetune GR00T-N1 on the G1 Block Stacking Dataset.\n",
    "\n",
    "## Step 1: Dataset\n",
    "\n",
    "Loading any dataset for finetuning can be done in 2 steps:\n",
    "- 1.1: Defining the modality configs and transforms for the dataset\n",
    "- 1.2: Loading the dataset using the `LeRobotSingleDataset` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: 1.0 Download the dataset\n",
    "\n",
    "- Download the dataset from: https://huggingface.co/datasets/unitreerobotics/G1_BlockStacking_Dataset\n",
    "- copy over the `examples/unitree_g1_blocks__modality.json` to the dataset `<DATASET_PATH>/meta/modality.json`\n",
    "  - This provides additional information about the state and action modalities to make it \"GR00T-compatible\"\n",
    "  - `cp examples/unitree_g1_blocks__modality.json datasets/G1_BlockStacking_Dataset/meta/modality.json`\n",
    "\n",
    "\n",
    "**Understanding the Modality Configs**\n",
    "\n",
    "This file provides detailed metadata about state and action modalities, enabling:\n",
    "\n",
    "- **Separate Data Storage and Interpretation:**\n",
    "  - **State and Action:** Stored as concatenated float32 arrays. The `modality.json` file supplies the metadata necessary to interpret these arrays as distinct, fine-grained fields with additional training information.\n",
    "  - **Video:** Stored as separate files, with the configuration file allowing them to be renamed to a standardized format.\n",
    "  - **Annotations:** Keeps track of all annotation fields. If there are no annotations, do not include the `annotation` field in the configuration file.\n",
    "- **Fine-Grained Splitting:** Divides the state and action arrays into more semantically meaningful fields.\n",
    "- **Clear Mapping:** Explicit mapping of data dimensions.\n",
    "- **Sophisticated Data Transformations:** Supports field-specific normalization and rotation transformations during training.\n",
    "\n",
    "#### Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"state\": {\n",
    "        \"<state_name>\": {\n",
    "            \"start\": <int>,         // Starting index in the state array\n",
    "            \"end\": <int>,           // Ending index in the state array\n",
    "        }\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"<action_name>\": {\n",
    "            \"start\": <int>,         // Starting index in the action array\n",
    "            \"end\": <int>,           // Ending index in the action array\n",
    "        }\n",
    "    },\n",
    "    \"video\": {\n",
    "        \"<video_name>\": {}  // Empty dictionary to maintain consistency with other modalities\n",
    "    },\n",
    "    \"annotation\": {\n",
    "        \"<annotation_name>\": {}  // Empty dictionary to maintain consistency with other modalities\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Example is shown in `getting_started/examples/unitree_g1_blocks__modality.json`. This file is located in the `meta` folder of the lerobot dataset.\n",
    "\n",
    "\n",
    "Generate the Stats (`meta/metadata.json`) by running the following command:\n",
    "```bash\n",
    "python scripts/load_dataset.py --data_path /datasets/G1_BlockStacking_Dataset/ --embodiment_tag new_embodiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.schema import EmbodimentTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./demo_data/g1\"  # change this to your dataset path\n",
    "embodiment_tag = EmbodimentTag.NEW_EMBODIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: 1.1 Modality configs and transforms\n",
    "\n",
    "Modality configs let you select which specific data streams to use for each input type (video, state, action, language, etc.) during finetuning, giving you precise control over which parts of your dataset are utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.dataset import ModalityConfig\n",
    "\n",
    "\n",
    "# select the modality keys you want to use for finetuning\n",
    "video_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"video.cam_right_high\"],\n",
    ")\n",
    "\n",
    "state_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"state.left_arm\", \"state.right_arm\", \"state.left_hand\", \"state.right_hand\"],\n",
    ")\n",
    "\n",
    "action_modality = ModalityConfig(\n",
    "    delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    modality_keys=[\"action.left_arm\", \"action.right_arm\", \"action.left_hand\", \"action.right_hand\"],\n",
    ")\n",
    "\n",
    "language_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"annotation.human.task_description\"],\n",
    ")\n",
    "\n",
    "modality_configs = {\n",
    "    \"video\": video_modality,\n",
    "    \"state\": state_modality,\n",
    "    \"action\": action_modality,\n",
    "    \"language\": language_modality,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "from gr00t.model.transforms import GR00TTransform\n",
    "\n",
    "\n",
    "# select the transforms you want to apply to the data\n",
    "to_apply_transforms = ComposedModalityTransform(\n",
    "    transforms=[\n",
    "        # video transforms\n",
    "        VideoToTensor(apply_to=video_modality.modality_keys, backend=\"torchvision\"),\n",
    "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95, backend=\"torchvision\"),\n",
    "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\", backend=\"torchvision\" ),\n",
    "        VideoColorJitter(apply_to=video_modality.modality_keys, brightness=0.3, contrast=0.4, saturation=0.5, hue=0.08, backend=\"torchvision\"),\n",
    "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "        # state transforms\n",
    "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
    "            \"state.left_arm\": \"min_max\",\n",
    "            \"state.right_arm\": \"min_max\",\n",
    "            \"state.left_hand\": \"min_max\",\n",
    "            \"state.right_hand\": \"min_max\",\n",
    "        }),\n",
    "\n",
    "        # action transforms\n",
    "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
    "            \"action.right_arm\": \"min_max\",\n",
    "            \"action.left_arm\": \"min_max\",\n",
    "            \"action.right_hand\": \"min_max\",\n",
    "            \"action.left_hand\": \"min_max\",\n",
    "        }),\n",
    "\n",
    "        # ConcatTransform\n",
    "        ConcatTransform(\n",
    "            video_concat_order=video_modality.modality_keys,\n",
    "            state_concat_order=state_modality.modality_keys,\n",
    "            action_concat_order=action_modality.modality_keys,\n",
    "        ),\n",
    "        # model-specific transform\n",
    "        GR00TTransform(\n",
    "            state_horizon=len(state_modality.delta_indices),\n",
    "            action_horizon=len(action_modality.delta_indices),\n",
    "            max_state_dim=64,\n",
    "            max_action_dim=32,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 Load the dataset\n",
    "\n",
    "First we will visualize the dataset and then load it using the `LeRobotSingleDataset` class. (without transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "\n",
    "train_dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to visualize the images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(train_dataset[0].keys())\n",
    "\n",
    "images = []\n",
    "for i in range(5):\n",
    "    image = train_dataset[i][\"video.cam_right_high\"][0]\n",
    "    # image is in HWC format, convert it to CHW format\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    images.append(image)   \n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i, image in enumerate(images):\n",
    "    axs[i].imshow(np.transpose(image, (1, 2, 0)))\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will initiate a dataset with our modality configs and transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    "    transforms=to_apply_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Notes**:\n",
    " - We use a cached dataloader to accelerate training speed. The cached dataloader loads all data into memory, which significantly improves training performance. However, if your dataset is large or you're experiencing out-of-memory (OOM) errors, you can switch to the standard lerobot dataloader (`gr00t.data.dataset.LeRobotSingleDataset`). It uses the same API as the cached dataloader, so you can switch back and forth without any changes to your code.\n",
    " - we use torchvision_av as the video backend, the video encoding is in av instead of standard h264\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the model\n",
    "\n",
    "The training process is done in 3 steps:\n",
    "- 2.1: Load the base model from HuggingFace or a local path\n",
    "- 2.2: Prepare training args\n",
    "- 2.3: Run the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Load the base model\n",
    "\n",
    "We'll use the `from_pretrained_for_tuning` method to load the model. This method allows us to specify which parts of the model to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.model.gr00t_n1 import GR00T_N1\n",
    "\n",
    "BASE_MODEL_PATH = \"nvidia/GR00T-N1-2B\"\n",
    "TUNE_LLM = False            # Whether to tune the LLM\n",
    "TUNE_VISUAL = True          # Whether to tune the visual encoder\n",
    "TUNE_PROJECTOR = True       # Whether to tune the projector\n",
    "TUNE_DIFFUSION_MODEL = True # Whether to tune the diffusion model\n",
    "\n",
    "model = GR00T_N1.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL_PATH,\n",
    "    tune_llm=TUNE_LLM,  # backbone's LLM\n",
    "    tune_visual=TUNE_VISUAL,  # backbone's vision tower\n",
    "    tune_projector=TUNE_PROJECTOR,  # action head's projector\n",
    "    tune_diffusion_model=TUNE_DIFFUSION_MODEL,  # action head's DiT\n",
    ")\n",
    "\n",
    "# Set the model's compute_dtype to bfloat16\n",
    "model.compute_dtype = \"bfloat16\"\n",
    "model.config.compute_dtype = \"bfloat16\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Prepare training args\n",
    "\n",
    "We use huggingface `TrainingArguments` to configure the training process. Here are the main parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"output/model/path\"    # CHANGE THIS ACCORDING TO YOUR LOCAL PATH\n",
    "per_device_train_batch_size = 8     # CHANGE THIS ACCORDING TO YOUR GPU MEMORY\n",
    "max_steps = 20                      # CHANGE THIS ACCORDING TO YOUR NEEDS\n",
    "report_to = \"wandb\"\n",
    "dataloader_num_workers = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=None,\n",
    "    remove_unused_columns=False,\n",
    "    deepspeed=\"\",\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.95,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10.0,\n",
    "    num_train_epochs=300,\n",
    "    max_steps=max_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=8,\n",
    "    report_to=report_to,\n",
    "    seed=42,\n",
    "    do_eval=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    ddp_bucket_cap_mb=100,\n",
    "    torch_compile_mode=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3 Initialize the training runner and run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.experiment.runner import TrainRunner\n",
    "\n",
    "experiment = TrainRunner(\n",
    "    train_dataset=train_dataset,\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    ")\n",
    "\n",
    "experiment.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the 1k offline validation results vs 10k offline validation results:\n",
    "\n",
    "**Finetuning Results on Unitree G1 Block Stacking Dataset:**\n",
    "\n",
    "| 1k Steps | 10k Steps |\n",
    "| --- | --- |\n",
    "| ![1k](../media/g1_ft_1k.png) | ![10k](../media/g1_ft_10k.png) |\n",
    "| MSE: 0.0181 | MSE: 0.0022 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
